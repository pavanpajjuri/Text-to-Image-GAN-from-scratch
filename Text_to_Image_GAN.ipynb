{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MgsrLt7nL1D_"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R8t_xYm-NpsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd8893d-237e-43ed-bb53-2e6ad9a092e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/pavan\n",
            "birds.hdf5    GAN\t\t    models\t results\t   Text_to_Image_GAN.ipynb\n",
            "data_util.py  generated_images_200  __pycache__  saved_models_200\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/pavan\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W1tIGmcqNr1L"
      },
      "outputs": [],
      "source": [
        "from data_util import Text2ImageDataset # Locally written code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Lz_GRaZNtl8"
      },
      "outputs": [],
      "source": [
        "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)   # (mean, std)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0,0.02)   # (mean = 1, std)\n",
        "        m.bias.data.fill_(0)  # Bias initiated with 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7dc2zg1bNxAK"
      },
      "outputs": [],
      "source": [
        "# Defining the Generator\n",
        "\n",
        "class G(nn.Module): # We introduce a class to define the generator.\n",
        "\n",
        "    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n",
        "        super(G, self).__init__() # We inherit from the nn.Module tools.\n",
        "\n",
        "        # Text Embedding Layer\n",
        "        self.text_embedding = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_out_dim),\n",
        "            nn.BatchNorm1d(embed_out_dim),\n",
        "            nn.LeakyReLU(0.2, inplace = True)\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
        "            nn.ConvTranspose2d(100 + embed_out_dim , 512, 4, 1, 0, bias= False), # We start with an inversed convolution. (in channels, out channels, kernel size, stride, padding) Bias is False as we are doing batch normalization in next step\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias= False), # We start with an inversed convolution. (in channels, out channels, kernel size, stride, padding) Bias is False as we are doing batch normalization in next step\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias= False), # We start with an inversed convolution. (in channels, out channels, kernel size, stride, padding) Bias is False as we are doing batch normalization in next step\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias= False), # We start with an inversed convolution. (in channels, out channels, kernel size, stride, padding) Bias is False as we are doing batch normalization in next step\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias= False), # We start with an inversed convolution. (in channels, out channels, kernel size, stride, padding) Bias is False as we are doing batch normalization in next step\n",
        "            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1 which is how the discriminator will again take in the values\n",
        "            )\n",
        "\n",
        "    def forward(self, noise, text): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.\n",
        "        # Process Text embeddings\n",
        "        text = self.text_embedding(text) # Input text embedding\n",
        "        text = text.view(text.shape[0], text.shape[1], 1, 1) # Reshaping to match the dimensions of noise\n",
        "\n",
        "        input = torch.cat([noise, text], 1) # Concat noise and text which will the input for main function\n",
        "        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n",
        "        return output # We return the output containing the generated images.\n",
        "\n",
        "\n",
        "# Defining the Discriminator\n",
        "\n",
        "class D(nn.Module): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
        "\n",
        "    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
        "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
        "\n",
        "        # Text Embedding Layer\n",
        "        self.text_embedding = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_out_dim),\n",
        "            nn.BatchNorm1d(embed_out_dim),\n",
        "            nn.LeakyReLU(0.2, inplace = True)\n",
        "            )\n",
        "\n",
        "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # Staring with Convolution network as the input is an image\n",
        "            nn.LeakyReLU(0.2, inplace = True), #Helps gradient flow (avoids dead neurons). inplace True since we dont need a new Tensor\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Conv2d(512 + embed_out_dim, 1, 4, 1, 0, bias = False), # Architecture takes the text embedding at the output again to check the text\n",
        "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
        "            )\n",
        "\n",
        "    def forward(self, image, text):\n",
        "\n",
        "        # Process image features first\n",
        "        image_features = self.main(image)\n",
        "\n",
        "        # Process text embedidngs\n",
        "        text = self.text_embedding(text) # Input text embedding\n",
        "        text = text.view(text.shape[0], text.shape[1], 1, 1) # Reshaping Text Embeddings\n",
        "        text = text.repeat(1,1, image_features.shape[2], image_features.shape[3]) # Reshaped text embeddings are repeated across the spatial dimensions of the image features\n",
        "\n",
        "\n",
        "        combined = torch.cat([image_features, text], 1) # Concat noise and text which will the input for main function\n",
        "        output = self.output(combined)\n",
        "        return output.view(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzKTcNkjNxwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97456c1-f584-46e7-84ab-f1cce3ba3605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n",
            "No of batches:  54\n",
            "Sample Keys: dict_keys(['right_images', 'right_embed', 'wrong_images', 'inter_embed', 'txt'])\n",
            "right_images: Tensor of shape torch.Size([3, 64, 64])\n",
            "right_embed: Tensor of shape torch.Size([1024])\n",
            "wrong_images: Tensor of shape torch.Size([3, 64, 64])\n",
            "inter_embed: Tensor of shape torch.Size([1024])\n",
            "txt: this particular bird has a belly that is yellow and white\n",
            "\n",
            "Epoch 1 [50/54] Loss_D: 5.5830 Loss_G: 44.7744 Time: 51.93\n",
            "Epoch 2 [50/54] Loss_D: 2.7510 Loss_G: 43.2776 Time: 51.23\n",
            "Epoch 3 [50/54] Loss_D: 2.1677 Loss_G: 43.3527 Time: 51.10\n",
            "Epoch 4 [50/54] Loss_D: 1.9950 Loss_G: 41.7741 Time: 50.91\n",
            "Epoch 5 [50/54] Loss_D: 1.9637 Loss_G: 42.2035 Time: 50.72\n",
            "Epoch 6 [50/54] Loss_D: 1.9497 Loss_G: 42.2878 Time: 51.07\n",
            "Epoch 7 [50/54] Loss_D: 1.8720 Loss_G: 42.4968 Time: 50.85\n",
            "Epoch 8 [50/54] Loss_D: 1.9561 Loss_G: 41.9665 Time: 51.26\n",
            "Epoch 9 [50/54] Loss_D: 2.0459 Loss_G: 41.8699 Time: 51.42\n",
            "Epoch 10 [50/54] Loss_D: 1.9072 Loss_G: 41.1456 Time: 51.29\n",
            "Epoch 11 [50/54] Loss_D: 1.8564 Loss_G: 41.5235 Time: 51.48\n",
            "Epoch 12 [50/54] Loss_D: 1.7687 Loss_G: 41.6152 Time: 51.36\n",
            "Epoch 13 [50/54] Loss_D: 1.7298 Loss_G: 42.7335 Time: 51.09\n",
            "Epoch 14 [50/54] Loss_D: 1.9053 Loss_G: 42.8768 Time: 50.82\n",
            "Epoch 15 [50/54] Loss_D: 1.9310 Loss_G: 42.7119 Time: 51.30\n",
            "Epoch 16 [50/54] Loss_D: 1.7484 Loss_G: 43.2704 Time: 51.64\n",
            "Epoch 17 [50/54] Loss_D: 1.9533 Loss_G: 42.3330 Time: 51.22\n",
            "Epoch 18 [50/54] Loss_D: 2.1408 Loss_G: 42.2726 Time: 52.42\n",
            "Epoch 19 [50/54] Loss_D: 1.9826 Loss_G: 42.0410 Time: 51.03\n",
            "Epoch 20 [50/54] Loss_D: 1.8218 Loss_G: 42.7273 Time: 51.49\n",
            "Epoch 21 [50/54] Loss_D: 1.6660 Loss_G: 42.6520 Time: 51.41\n",
            "Epoch 22 [50/54] Loss_D: 1.8081 Loss_G: 43.1212 Time: 51.65\n",
            "Epoch 23 [50/54] Loss_D: 1.7070 Loss_G: 42.2205 Time: 51.74\n",
            "Epoch 24 [50/54] Loss_D: 1.6669 Loss_G: 43.7919 Time: 51.32\n",
            "Epoch 25 [50/54] Loss_D: 1.6980 Loss_G: 41.7815 Time: 51.74\n",
            "Epoch 26 [50/54] Loss_D: 1.4506 Loss_G: 43.0786 Time: 51.52\n",
            "Epoch 27 [50/54] Loss_D: 1.4202 Loss_G: 44.2018 Time: 51.62\n",
            "Epoch 28 [50/54] Loss_D: 1.8026 Loss_G: 41.7964 Time: 51.27\n",
            "Epoch 29 [50/54] Loss_D: 1.7726 Loss_G: 43.2753 Time: 51.29\n",
            "Epoch 30 [50/54] Loss_D: 1.5600 Loss_G: 43.8469 Time: 51.81\n",
            "Epoch 31 [50/54] Loss_D: 1.7957 Loss_G: 42.0116 Time: 51.06\n",
            "Epoch 32 [50/54] Loss_D: 1.5816 Loss_G: 42.1865 Time: 50.54\n",
            "Epoch 33 [50/54] Loss_D: 1.5967 Loss_G: 47.3647 Time: 51.90\n",
            "Epoch 34 [50/54] Loss_D: 1.5531 Loss_G: 50.3527 Time: 51.53\n",
            "Epoch 35 [50/54] Loss_D: 1.5161 Loss_G: 48.1466 Time: 51.54\n",
            "Epoch 36 [50/54] Loss_D: 1.4240 Loss_G: 47.4667 Time: 51.09\n",
            "Epoch 37 [50/54] Loss_D: 1.4493 Loss_G: 47.9639 Time: 51.09\n",
            "Epoch 38 [50/54] Loss_D: 1.4327 Loss_G: 47.6409 Time: 50.86\n",
            "Epoch 39 [50/54] Loss_D: 1.3980 Loss_G: 49.2082 Time: 51.10\n",
            "Epoch 40 [50/54] Loss_D: 1.4387 Loss_G: 48.0753 Time: 51.38\n",
            "Epoch 41 [50/54] Loss_D: 1.3945 Loss_G: 49.2060 Time: 50.92\n",
            "Epoch 42 [50/54] Loss_D: 1.4495 Loss_G: 51.0268 Time: 50.89\n",
            "Epoch 43 [50/54] Loss_D: 1.3940 Loss_G: 49.3718 Time: 51.18\n",
            "Epoch 44 [50/54] Loss_D: 1.4189 Loss_G: 50.8072 Time: 50.80\n",
            "Epoch 45 [50/54] Loss_D: 1.4457 Loss_G: 49.3059 Time: 51.94\n",
            "Epoch 46 [50/54] Loss_D: 1.3842 Loss_G: 49.3304 Time: 50.82\n",
            "Epoch 47 [50/54] Loss_D: 1.4562 Loss_G: 48.0639 Time: 50.86\n",
            "Epoch 48 [50/54] Loss_D: 1.3988 Loss_G: 49.0840 Time: 50.64\n",
            "Epoch 49 [50/54] Loss_D: 1.3638 Loss_G: 50.0734 Time: 50.87\n",
            "Epoch 50 [50/54] Loss_D: 1.3911 Loss_G: 48.4112 Time: 50.95\n",
            "Epoch 51 [50/54] Loss_D: 1.3888 Loss_G: 49.0223 Time: 50.89\n",
            "Epoch 52 [50/54] Loss_D: 1.4149 Loss_G: 49.7545 Time: 51.60\n",
            "Epoch 53 [50/54] Loss_D: 1.5030 Loss_G: 50.2620 Time: 50.91\n"
          ]
        }
      ],
      "source": [
        "# Hyperparamaters defining\n",
        "batchSize = 512\n",
        "imageSize = 64\n",
        "embed_dim = 1024 # Dimension of text embeddings\n",
        "embed_out_dim = 128 # Output Dimension of text embeddings after processing\n",
        "l1_coef = 50  # Weight for L1 loss\n",
        "l2_coef = 100  # Weight for L2 loss\n",
        "numWorkers = 8\n",
        "\n",
        "epochs = 300\n",
        "initial_epoch = 0  # Resume training from epoch 90\n",
        "\n",
        "log_interval = 50\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Directory setup\n",
        "    output_save_path = './generated_images/'\n",
        "    model_save_path = './saved_models/'\n",
        "    os.makedirs(output_save_path, exist_ok=True)\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # If using Nvidia GPU\n",
        "    # device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\") # If using Mac GPU\n",
        "    print(\"Using Device:\", device)\n",
        "\n",
        "    #dataset = dset.CIFAR10(root = \"./data\", download= True, transform=transform)\n",
        "    dataset = Text2ImageDataset('birds.hdf5',split=0) # split { 0: train, 1: validation, 2: test }\n",
        "    \"\"\"\n",
        "    To get the birds.hdf5 data:\n",
        "       : First download the data birds data from here https://drive.google.com/file/d/0B0ywwgffWnLLZW9uVHNjb2JmNlE/view?resourcekey=0-8y2UVmBHAlG26HafWYNoFQ\n",
        "       : In the config.yaml path change the path accordingly\n",
        "       : run convert_cub_to_hdf5_script.py script which should output the hdf5 file\n",
        "\n",
        "    Hd5 file taxonomy `\n",
        "\n",
        "    split (train | valid | test )\n",
        "    example_name\n",
        "    'name'\n",
        "    'img'\n",
        "    'embeddings'\n",
        "    'class'\n",
        "    'txt'\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Using dataLoader to get the images of the training set batch by batch.\n",
        "    # A higher num_workers (>1) enables faster data loading by using multiple CPU cores.\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batchSize, shuffle = True, num_workers= numWorkers, pin_memory = True)\n",
        "    print(\"No of batches: \",len(dataloader))\n",
        "\n",
        "    # Print a Sample Input\n",
        "    sample = dataset[0]  # Get the first item from the dataset\n",
        "    print(\"Sample Keys:\", sample.keys())\n",
        "\n",
        "    for key, value in sample.items():\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            print(f\"{key}: Tensor of shape {value.shape}\")\n",
        "        elif isinstance(value, str):\n",
        "            print(f\"{key}: {value}\")\n",
        "        elif isinstance(value, int):\n",
        "            print(f\"{key}: {value}\")\n",
        "        elif isinstance(value, np.ndarray):\n",
        "            print(f\"{key}: Numpy array of shape {value.shape}\")\n",
        "        else:\n",
        "            print(f\"{key}: {type(value)}\")\n",
        "\n",
        "    # Creating the Generator\n",
        "    netG = G().to(device) # We create the generator object.\n",
        "    netG.apply(weights_init) # We initialize all the weights of its neural network.\n",
        "\n",
        "    # Creating the Discriminator'\n",
        "    netD = D().to(device)\n",
        "    netD.apply(weights_init)\n",
        "\n",
        "\n",
        "    # Training the DC-GANs\n",
        "\n",
        "    # Loss functions\n",
        "    criterion = nn.BCELoss() # We create a Binary Cross Entropy criterion object that will measure the error between the prediction and the target.\n",
        "    l1_loss = nn.L1Loss() # L1 Loss (Pixel-wise) Improves clarity\n",
        "    l2_loss = nn.MSELoss() # L2 Loss (Feature-Wise) Improves Features described in the image\n",
        "\n",
        "    #Optimizers\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the Generator.\n",
        "\n",
        "    # # Load saved checkpoints to resume training\n",
        "    # netG.load_state_dict(torch.load(os.path.join(model_save_path, 'generator_epoch_089.pth'), map_location=device))\n",
        "    # netD.load_state_dict(torch.load(os.path.join(model_save_path, 'discriminator_epoch_089.pth'), map_location=device))\n",
        "\n",
        "    # print(f\"Resumed training from epoch {initial_epoch}\")\n",
        "\n",
        "    # Lists to store losses\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(initial_epoch, epochs):\n",
        "\n",
        "        batch_time = time.time()  # Start time for the epoch\n",
        "        for i, batch in enumerate(dataloader): # iterte over the images of the dataset\n",
        "\n",
        "            # Load real images and their text embeddings\n",
        "            # We get a real image, wrong image  and the respective text embedding of the dataset which will be used to train the discriminator.\n",
        "            real_images = batch['right_images'].to(device)\n",
        "            wrong_images = batch['wrong_images'].to(device)\n",
        "            text_embeddings = batch['right_embed'].to(device)\n",
        "            batch_size = real_images.size(0)\n",
        "\n",
        "            real_labels = torch.ones(real_images.size(0), device = device)\n",
        "            fake_labels = torch.zeros(real_images.size(0), device = device)\n",
        "\n",
        "            # 1st Step : Updating the weights of the Discriminator and Generator\n",
        "            netD.zero_grad() # Important : We initialize to 0 the gradients of the discriminator with respect to the weights.\n",
        "\n",
        "            # Train the Discriminator with some real images of the dataset\n",
        "            output_real = netD(real_images, text_embeddings) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
        "            errD_real = criterion(output_real, real_labels) # We compute the loss between the predictions (output) and the target (equal to 1).\n",
        "\n",
        "            # Train the Discriminator with mismatched text image pairs so it knows wrong ones\n",
        "            output_wrong = netD(wrong_images, text_embeddings) # We forward propagate this wrong image into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
        "            errD_wrong = criterion(output_wrong, fake_labels) # We compute the loss between the predictions (output) and the target (equal to 0).\n",
        "\n",
        "            # Train the Discriminator with some fake images\n",
        "            noise = torch.randn(real_images.size(0), 100, 1, 1, device = device) # We make a random input vector (noise - normal distributed) [called latent vector] of the generator.\n",
        "            fake_images = netG(noise, text_embeddings) # We forward propagate this noise and text embeddings into the neural network of the generator to get some fake generated images.\n",
        "            output_fake = netD(fake_images.detach(), text_embeddings) # We forward propagate the fake generated images and the text embedding into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
        "            errD_fake = criterion(output_fake, fake_labels) # We compute the loss between the prediction (output) and the target (equal to 0).\n",
        "\n",
        "            # Backpropagating the total error\n",
        "            errD = errD_real + errD_wrong + errD_fake\n",
        "            errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.\n",
        "            optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.\n",
        "\n",
        "            # 2nd Step: Updating the weights of the neural network of the generator\n",
        "            netG.zero_grad()  # Important : We initialize to 0 the gradients of the generator with respect to the weights.\n",
        "\n",
        "            output = netD(fake_images, text_embeddings) # We forward propagate the fake generated images and text images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
        "            errG_bce = criterion(output, real_labels) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).\n",
        "\n",
        "            errG_l1 = l1_coef * l1_loss(fake_images, real_images)\n",
        "            errG_l2 = l2_coef * l2_loss(fake_images, real_images)\n",
        "\n",
        "            errG = errG_bce + errG_l1 + errG_l2\n",
        "\n",
        "            errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.\n",
        "            optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.\n",
        "\n",
        "            # Store losses\n",
        "            D_losses.append(errD.item())\n",
        "            G_losses.append(errG.item())\n",
        "\n",
        "            # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
        "            # Progress based on log_interval\n",
        "            if (i + 1) % log_interval == 0 and i > 0:\n",
        "                print('Epoch {} [{}/{}] Loss_D: {:.4f} Loss_G: {:.4f} Time: {:.2f}'.format(\n",
        "                    epoch + 1, i + 1, len(dataloader),\n",
        "                    errD.mean().item(),\n",
        "                    errG.mean().item(),\n",
        "                    time.time() - batch_time))\n",
        "\n",
        "            # Save generator output after every 10 epochs\n",
        "            if i == len(dataloader) - 1 and ((epoch + 1) % 10 == 0 or epoch == 0):\n",
        "                viz_sample = torch.cat((real_images[:32], fake_images[:32]), 0)\n",
        "                vutils.save_image(viz_sample,\n",
        "                                  os.path.join(output_save_path, 'output_epoch_{:03d}.png'.format(epoch + 1)),\n",
        "                                  nrow=8, normalize=True)\n",
        "\n",
        "                # Save models for every 10 epochs\n",
        "                torch.save(netG.state_dict(), os.path.join(model_save_path, 'generator_epoch_%03d.pth' % epoch))\n",
        "                torch.save(netD.state_dict(), os.path.join(model_save_path, 'discriminator_epoch_%03d.pth' % epoch))\n",
        "\n",
        "    # Save final models\n",
        "    torch.save(netG.state_dict(), os.path.join(model_save_path, 'generator_final.pth'))\n",
        "    torch.save(netD.state_dict(), os.path.join(model_save_path, 'discriminator_final.pth'))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(G_losses, label=\"Generator Loss\")\n",
        "    plt.plot(D_losses, label=\"Discriminator Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(output_save_path, 'loss_plot.png'))\n",
        "    plt.show()\n",
        "\n",
        "    print('Total time for Execution: {}'.format(time.time()-start_time))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}